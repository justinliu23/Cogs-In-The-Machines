{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee3e60-a34c-4e50-9599-18f67f5943ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unbiased\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of biases in order\n",
    "biases = [\"self_diagnosis\", \"recency\", \"confirmation\", \"confirmation_v2\",\n",
    "          \"frequency\", \"cultural\", \"status_quo\", \"false_consensus\", \"false_consensus_v2\"]\n",
    "\n",
    "# Step 1: Read in the two cogbias output JSON files into dictionaries\n",
    "with open(\"results/incorrect_base.json\", \"r\") as f:\n",
    "    cogbias_incorrect_results = json.load(f)\n",
    "\n",
    "with open(\"results/correct_base.json\", \"r\") as f:\n",
    "    cogbias_correct_results = json.load(f)\n",
    "\n",
    "# Step 2: Assign biases cyclically to each entry in these datasets\n",
    "def assign_biases_cyclically(results, biases_list):\n",
    "    for i, entry in enumerate(results):\n",
    "        entry['bias'] = biases_list[i % len(biases_list)]\n",
    "    return results\n",
    "\n",
    "# Assign biases to the results\n",
    "cogbias_incorrect_results = assign_biases_cyclically(cogbias_incorrect_results, biases)\n",
    "cogbias_correct_results = assign_biases_cyclically(cogbias_correct_results, biases)\n",
    "\n",
    "# Step 3: Output them back to separate JSON files with biases included\n",
    "with open(\"cogbias_incorrect_results_with_bias.json\", \"w\") as f:\n",
    "    json.dump(cogbias_incorrect_results, f, indent=4)\n",
    "\n",
    "with open(\"cogbias_correct_results_with_bias.json\", \"w\") as f:\n",
    "    json.dump(cogbias_correct_results, f, indent=4)\n",
    "\n",
    "print(\"Results with biases saved to 'cogbias_incorrect_results_with_bias.json' and 'cogbias_correct_results_with_bias.json'.\")\n",
    "\n",
    "# Step 4: Group by bias for each dataset separately and count correct responses\n",
    "def prepare_data_for_plotting(results, dataset_name):\n",
    "    data = []\n",
    "    count = 0\n",
    "    for entry in results:\n",
    "        correct_answer = entry.get('correct_answer', '').strip()\n",
    "        response = entry.get('response', '').strip()\n",
    "        bias = entry.get('bias', 'unknown')\n",
    "        is_correct = (correct_answer == response)\n",
    "        if response not in ['A','B','C','D','E']:\n",
    "            count += 1\n",
    "            print(response)\n",
    "            \n",
    "        data.append({'Bias': bias, 'Is Correct': is_correct, 'Dataset': dataset_name})\n",
    "    df = pd.DataFrame(data)\n",
    "    print(count)\n",
    "    return df\n",
    "\n",
    "# Prepare data for both datasets\n",
    "df_incorrect = prepare_data_for_plotting(cogbias_incorrect_results, 'Incorrect Bias')\n",
    "df_correct = prepare_data_for_plotting(cogbias_correct_results, 'Correct Bias')\n",
    "\n",
    "# Combine the dataframes\n",
    "df_combined = pd.concat([df_incorrect, df_correct], ignore_index=True)\n",
    "\n",
    "# Count the number of correct responses per bias for each dataset\n",
    "accuracy_df = df_combined.groupby(['Dataset', 'Bias'])['Is Correct'].mean().reset_index(name='Accuracy')\n",
    "\n",
    "# Calculate overall accuracy for the unbiased dataset\n",
    "with open(\"results/unbiased_base.json\", \"r\") as f:\n",
    "    cogbias_unbiased_results = json.load(f)\n",
    "\n",
    "# Compute unbiased accuracy\n",
    "unbiased_correct = sum(1 for entry in cogbias_unbiased_results if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "unbiased_total = len(cogbias_unbiased_results)\n",
    "unbiased_accuracy = unbiased_correct / unbiased_total\n",
    "\n",
    "print(f\"Unbiased Dataset Accuracy: {unbiased_accuracy:.2%}\")\n",
    "\n",
    "# Step 5: Create a pretty Seaborn graph comparing the accuracies\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=accuracy_df, x='Bias', y='Accuracy', hue='Dataset', palette='Set2')\n",
    "plt.axhline(y=unbiased_accuracy, color='red', linestyle='--', label=f'Unbiased Accuracy ({unbiased_accuracy:.2%})')\n",
    "plt.title('LLaMA 3.1 8B Accuracy per Cognitive Bias in Clinical Decisions', fontsize=18)\n",
    "plt.xlabel('Bias', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Dataset', fontsize=12, title_fontsize=14, loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('bias_accuracy_comparison_with_unbiased_line.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b225bd-ca8d-436d-8b65-6b0eaed7101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of biases in order\n",
    "biases = [\"self_diagnosis\", \"recency\", \"confirmation\", \"confirmation_v2\",\n",
    "          \"frequency\", \"cultural\", \"status_quo\", \"false_consensus\", \"false_consensus_v2\"]\n",
    "\n",
    "# Step 1: Read in the two cogbias output JSON files into dictionaries\n",
    "with open(\"results/incorrect_base_prompt.json\", \"r\") as f:\n",
    "    cogbias_incorrect_results = json.load(f)\n",
    "\n",
    "with open(\"results/correct_base_prompt.json\", \"r\") as f:\n",
    "    cogbias_correct_results = json.load(f)\n",
    "\n",
    "# Step 2: Assign biases cyclically to each entry in these datasets\n",
    "def assign_biases_cyclically(results, biases_list):\n",
    "    for i, entry in enumerate(results):\n",
    "        entry['bias'] = biases_list[i % len(biases_list)]\n",
    "    return results\n",
    "\n",
    "# Assign biases to the results\n",
    "cogbias_incorrect_results = assign_biases_cyclically(cogbias_incorrect_results, biases)\n",
    "cogbias_correct_results = assign_biases_cyclically(cogbias_correct_results, biases)\n",
    "\n",
    "# Step 3: Output them back to separate JSON files with biases included\n",
    "with open(\"cogbias_incorrect_results_with_bias_prompt.json\", \"w\") as f:\n",
    "    json.dump(cogbias_incorrect_results, f, indent=4)\n",
    "\n",
    "with open(\"cogbias_correct_results_with_bias_prompt.json\", \"w\") as f:\n",
    "    json.dump(cogbias_correct_results, f, indent=4)\n",
    "\n",
    "print(\"Results with biases saved to 'cogbias_incorrect_results_with_bias_prompt.json' and 'cogbias_correct_results_with_bias_prompt.json'.\")\n",
    "\n",
    "# Step 4: Group by bias for each dataset separately and count correct responses\n",
    "def prepare_data_for_plotting(results, dataset_name):\n",
    "    data = []\n",
    "    count = 0\n",
    "    for entry in results:\n",
    "        correct_answer = entry.get('correct_answer', '').strip()\n",
    "        response = entry.get('response', '').strip()\n",
    "        bias = entry.get('bias', 'unknown')\n",
    "        is_correct = (correct_answer == response)\n",
    "        if response not in ['A','B','C','D','E']:\n",
    "            count += 1\n",
    "            print(response)\n",
    "            \n",
    "        data.append({'Bias': bias, 'Is Correct': is_correct, 'Dataset': dataset_name})\n",
    "    df = pd.DataFrame(data)\n",
    "    print(count)\n",
    "    return df\n",
    "\n",
    "# Prepare data for both datasets\n",
    "df_incorrect = prepare_data_for_plotting(cogbias_incorrect_results, 'Incorrect Bias')\n",
    "df_correct = prepare_data_for_plotting(cogbias_correct_results, 'Correct Bias')\n",
    "\n",
    "# Combine the dataframes\n",
    "df_combined = pd.concat([df_incorrect, df_correct], ignore_index=True)\n",
    "\n",
    "# Count the number of correct responses per bias for each dataset\n",
    "accuracy_df = df_combined.groupby(['Dataset', 'Bias'])['Is Correct'].mean().reset_index(name='Accuracy')\n",
    "\n",
    "# Calculate overall accuracy for the unbiased dataset\n",
    "with open(\"results/unbiased_base_prompt.json\", \"r\") as f:\n",
    "    cogbias_unbiased_results = json.load(f)\n",
    "\n",
    "# Compute unbiased accuracy\n",
    "unbiased_correct = sum(1 for entry in cogbias_unbiased_results if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "unbiased_total = len(cogbias_unbiased_results)\n",
    "unbiased_accuracy = unbiased_correct / unbiased_total\n",
    "\n",
    "print(f\"Unbiased Dataset Accuracy: {unbiased_accuracy:.2%}\")\n",
    "\n",
    "# Step 5: Create a pretty Seaborn graph comparing the accuracies\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=accuracy_df, x='Bias', y='Accuracy', hue='Dataset', palette='Set2')\n",
    "plt.axhline(y=unbiased_accuracy, color='red', linestyle='--', label=f'Unbiased Accuracy ({unbiased_accuracy:.2%})')\n",
    "plt.title('LLaMA 3.1 8B Accuracy per Cognitive Bias in Clinical Decisions', fontsize=18)\n",
    "plt.xlabel('Bias', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Dataset', fontsize=12, title_fontsize=14, loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('bias_accuracy_comparison_with_unbiased_line.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a088a3-d894-485b-891b-894f24246b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuned lora\n",
    "\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of biases in order\n",
    "biases = [\"self_diagnosis\", \"recency\", \"confirmation\", \"confirmation_v2\",\n",
    "          \"frequency\", \"cultural\", \"status_quo\", \"false_consensus\", \"false_consensus_v2\"]\n",
    "\n",
    "# Step 1: Read in the two cogbias output JSON files into dictionaries\n",
    "with open(\"results/incorrect_finetuned.json\", \"r\") as f:\n",
    "    cogbias_incorrect_results = json.load(f)\n",
    "\n",
    "with open(\"results/correct_finetuned\", \"r\") as f:\n",
    "    cogbias_correct_results = json.load(f)\n",
    "\n",
    "# Step 2: Assign biases cyclically to each entry in these datasets\n",
    "def assign_biases_cyclically(results, biases_list):\n",
    "    for i, entry in enumerate(results):\n",
    "        entry['bias'] = biases_list[i % len(biases_list)]\n",
    "    return results\n",
    "\n",
    "# Assign biases to the results\n",
    "cogbias_incorrect_results = assign_biases_cyclically(cogbias_incorrect_results, biases)\n",
    "cogbias_correct_results = assign_biases_cyclically(cogbias_correct_results, biases)\n",
    "\n",
    "# Step 3: Output them back to separate JSON files with biases included\n",
    "with open(\"cogbias_incorrect_results_with_bias_finetuned_round2.json\", \"w\") as f:\n",
    "    json.dump(cogbias_incorrect_results, f, indent=4)\n",
    "\n",
    "with open(\"cogbias_correct_results_with_bias_finetuned_round2.json\", \"w\") as f:\n",
    "    json.dump(cogbias_correct_results, f, indent=4)\n",
    "\n",
    "print(\"Results with biases saved to 'cogbias_incorrect_results_with_bias_finetuned_round2.json' and 'cogbias_correct_results_with_bias_finetuned_round2.json'.\")\n",
    "\n",
    "# Step 4: Group by bias for each dataset separately and count correct responses\n",
    "def prepare_data_for_plotting(results, dataset_name):\n",
    "    data = []\n",
    "    for entry in results:\n",
    "        correct_answer = entry.get('correct_answer', '').strip()\n",
    "        response = entry.get('response', '').strip()\n",
    "        bias = entry.get('bias', 'unknown')\n",
    "        is_correct = (correct_answer == response)\n",
    "        data.append({'Bias': bias, 'Is Correct': is_correct, 'Dataset': dataset_name})\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Prepare data for both datasets\n",
    "df_incorrect = prepare_data_for_plotting(cogbias_incorrect_results, 'Incorrect Bias')\n",
    "df_correct = prepare_data_for_plotting(cogbias_correct_results, 'Correct Bias')\n",
    "\n",
    "# Combine the dataframes\n",
    "df_combined = pd.concat([df_incorrect, df_correct], ignore_index=True)\n",
    "\n",
    "# Count the number of correct responses per bias for each dataset\n",
    "accuracy_df = df_combined.groupby(['Dataset', 'Bias'])['Is Correct'].mean().reset_index(name='Accuracy')\n",
    "\n",
    "# Calculate overall accuracy for the unbiased dataset\n",
    "with open(\"results/unbiased_finetuned.json\", \"r\") as f:\n",
    "    cogbias_unbiased_results = json.load(f)\n",
    "\n",
    "# Compute unbiased accuracy\n",
    "unbiased_correct = sum(1 for entry in cogbias_unbiased_results if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "unbiased_total = len(cogbias_unbiased_results)\n",
    "unbiased_accuracy = unbiased_correct / unbiased_total\n",
    "\n",
    "print(f\"Unbiased Dataset Accuracy: {unbiased_accuracy:.2%}\")\n",
    "\n",
    "# Step 5: Create a pretty Seaborn graph comparing the accuracies\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=accuracy_df, x='Bias', y='Accuracy', hue='Dataset', palette='Set2')\n",
    "plt.axhline(y=unbiased_accuracy, color='red', linestyle='--', label=f'Unbiased Accuracy ({unbiased_accuracy:.2%})')\n",
    "plt.title('Fine-tuned LLaMA 3.1 8B Accuracy per Cognitive Bias in Clinical Decisions', fontsize=18)\n",
    "plt.xlabel('Bias', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Dataset', fontsize=12, title_fontsize=14, loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('bias_accuracy_comparison_with_unbiased_line_finetuned_round2.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b232fd-354b-44ab-9914-30192bc8c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuned lora prompt\n",
    "\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of biases in order\n",
    "biases = [\"self_diagnosis\", \"recency\", \"confirmation\", \"confirmation_v2\",\n",
    "          \"frequency\", \"cultural\", \"status_quo\", \"false_consensus\", \"false_consensus_v2\"]\n",
    "\n",
    "# Step 1: Read in the two cogbias output JSON files into dictionaries\n",
    "with open(\"results/incorrect_finetuned_prompt\", \"r\") as f:\n",
    "    cogbias_incorrect_results = json.load(f)\n",
    "\n",
    "with open(\"results/correct_finetuned_prompt\", \"r\") as f:\n",
    "    cogbias_correct_results = json.load(f)\n",
    "\n",
    "# Step 2: Assign biases cyclically to each entry in these datasets\n",
    "def assign_biases_cyclically(results, biases_list):\n",
    "    for i, entry in enumerate(results):\n",
    "        entry['bias'] = biases_list[i % len(biases_list)]\n",
    "    return results\n",
    "\n",
    "# Assign biases to the results\n",
    "cogbias_incorrect_results = assign_biases_cyclically(cogbias_incorrect_results, biases)\n",
    "cogbias_correct_results = assign_biases_cyclically(cogbias_correct_results, biases)\n",
    "\n",
    "# Step 3: Output them back to separate JSON files with biases included\n",
    "# with open(\"cogbias_incorrect_results_with_bias_finetuned_round2_prompt.json\", \"w\") as f:\n",
    "#     json.dump(cogbias_incorrect_results, f, indent=4)\n",
    "\n",
    "# with open(\"cogbias_correct_results_with_bias_finetuned_round2_prompt.json\", \"w\") as f:\n",
    "#     json.dump(cogbias_correct_results, f, indent=4)\n",
    "\n",
    "print(\"Results with biases saved to 'cogbias_incorrect_results_with_bias_finetuned_round2.json' and 'cogbias_correct_results_with_bias_finetuned_round2.json'.\")\n",
    "\n",
    "# Step 4: Group by bias for each dataset separately and count correct responses\n",
    "def prepare_data_for_plotting(results, dataset_name):\n",
    "    data = []\n",
    "    for entry in results:\n",
    "        correct_answer = entry.get('correct_answer', '').strip()\n",
    "        response = entry.get('response', '').strip()\n",
    "        bias = entry.get('bias', 'unknown')\n",
    "        is_correct = (correct_answer == response)\n",
    "        data.append({'Bias': bias, 'Is Correct': is_correct, 'Dataset': dataset_name})\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Prepare data for both datasets\n",
    "df_incorrect = prepare_data_for_plotting(cogbias_incorrect_results, 'Incorrect Bias')\n",
    "df_correct = prepare_data_for_plotting(cogbias_correct_results, 'Correct Bias')\n",
    "\n",
    "# Combine the dataframes\n",
    "df_combined = pd.concat([df_incorrect, df_correct], ignore_index=True)\n",
    "\n",
    "# Count the number of correct responses per bias for each dataset\n",
    "accuracy_df = df_combined.groupby(['Dataset', 'Bias'])['Is Correct'].mean().reset_index(name='Accuracy')\n",
    "\n",
    "# Calculate overall accuracy for the unbiased dataset\n",
    "with open(\"results/unbiased_finetuned_prompt.json\", \"r\") as f:\n",
    "    cogbias_unbiased_results = json.load(f)\n",
    "\n",
    "# Compute unbiased accuracy\n",
    "unbiased_correct = sum(1 for entry in cogbias_unbiased_results if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "unbiased_total = len(cogbias_unbiased_results)\n",
    "unbiased_accuracy = unbiased_correct / unbiased_total\n",
    "\n",
    "print(f\"Unbiased Dataset Accuracy: {unbiased_accuracy:.2%}\")\n",
    "\n",
    "# Step 5: Create a pretty Seaborn graph comparing the accuracies\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=accuracy_df, x='Bias', y='Accuracy', hue='Dataset', palette='Set2')\n",
    "plt.axhline(y=unbiased_accuracy, color='red', linestyle='--', label=f'Unbiased Accuracy ({unbiased_accuracy:.2%})')\n",
    "plt.title('Fine-tuned LLaMA 3.1 8B Accuracy per Cognitive Bias in Clinical Decisions', fontsize=18)\n",
    "plt.xlabel('Bias', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Dataset', fontsize=12, title_fontsize=14, loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('bias_accuracy_comparison_with_unbiased_line_finetuned_round2.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd99c5-6c1d-485d-84f0-8db87dd6b4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# List of biases in a fixed order for plotting\n",
    "biases = [\"self_diagnosis\", \"recency\", \"confirmation\", \"confirmation_v2\",\n",
    "          \"frequency\", \"cultural\", \"status_quo\", \"false_consensus\", \"false_consensus_v2\"]\n",
    "\n",
    "#-----------------------------\n",
    "# Load All Results Files\n",
    "#-----------------------------\n",
    "\n",
    "# Original, base model, biased scenarios\n",
    "with open(\"cogbias_incorrect_results_with_bias.json\", \"r\") as f:\n",
    "    orig_incorrect_results = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias.json\", \"r\") as f:\n",
    "    orig_correct_results = json.load(f)\n",
    "\n",
    "# Fine-tuned, biased scenarios\n",
    "with open(\"cogbias_incorrect_results_with_bias_finetuned_round2.json\", \"r\") as f:\n",
    "    ft_incorrect_results = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias_finetuned_round2.json\", \"r\") as f:\n",
    "    ft_correct_results = json.load(f)\n",
    "\n",
    "# Original, base model, biased scenarios with prompt\n",
    "with open(\"cogbias_incorrect_results_with_bias_prompt.json\", \"r\") as f:\n",
    "    orig_incorrect_results_prompt = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias_prompt.json\", \"r\") as f:\n",
    "    orig_correct_results_prompt = json.load(f)\n",
    "\n",
    "# Fine-tuned, biased scenarios with prompt -- INCLUDED BUT COMMENTED OUT\n",
    "with open(\"cogbias_incorrect_results_with_bias_finetuned_round2_prompt.json\", \"r\") as f:\n",
    "    ft_incorrect_results_prompt = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias_finetuned_round2_prompt.json\", \"r\") as f:\n",
    "    ft_correct_results_prompt = json.load(f)\n",
    "\n",
    "# Unbiased (original and fine-tuned)\n",
    "with open(\"results/unbiased_base.json\", \"r\") as f:\n",
    "    orig_unbiased_results = json.load(f)\n",
    "with open(\"results/unbiased_finetuned.json\", \"r\") as f:\n",
    "    ft_unbiased_results = json.load(f)\n",
    "\n",
    "# Unbiased with prompt (original and fine-tuned)\n",
    "with open(\"results/unbiased_base_prompt.json\", \"r\") as f:\n",
    "    orig_unbiased_results_prompt = json.load(f)\n",
    "with open(\"results/unbiased_finetuned_prompt.json\", \"r\") as f:\n",
    "    ft_unbiased_results_prompt = json.load(f)\n",
    "\n",
    "#-----------------------------\n",
    "# Helper Function\n",
    "#-----------------------------\n",
    "def prepare_data_for_plotting(results, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    results: a list of entries, each with 'correct_answer', 'response', and 'bias' fields\n",
    "    model_name: 'Base' or 'Fine-tuned'\n",
    "    dataset_name: Something like 'Correct Bias', 'Incorrect Bias', 'Correct Bias Prompt', etc.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for entry in results:\n",
    "        correct_answer = entry.get('correct_answer', '').strip()\n",
    "        response = entry.get('response', '').strip()\n",
    "        bias = entry.get('bias', 'unknown')\n",
    "        is_correct = (correct_answer == response)\n",
    "        data.append({'Bias': bias, 'Is Correct': is_correct, 'Model': model_name, 'Dataset': dataset_name})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "#-----------------------------\n",
    "# Prepare DataFrames\n",
    "#-----------------------------\n",
    "\n",
    "# Without prompt\n",
    "df_base_incorrect = prepare_data_for_plotting(orig_incorrect_results, \"Base\", \"Incorrect Bias\")\n",
    "df_base_correct = prepare_data_for_plotting(orig_correct_results, \"Base\", \"Correct Bias\")\n",
    "df_ft_incorrect = prepare_data_for_plotting(ft_incorrect_results, \"Fine-tuned\", \"Incorrect Bias\")\n",
    "df_ft_correct = prepare_data_for_plotting(ft_correct_results, \"Fine-tuned\", \"Correct Bias\")\n",
    "\n",
    "# With prompt\n",
    "df_base_incorrect_prompt = prepare_data_for_plotting(orig_incorrect_results_prompt, \"Base\", \"Incorrect Bias Prompt\")\n",
    "df_base_correct_prompt = prepare_data_for_plotting(orig_correct_results_prompt, \"Base\", \"Correct Bias Prompt\")\n",
    "\n",
    "# Fine-tuned with prompt DataFrames\n",
    "df_ft_incorrect_prompt = prepare_data_for_plotting(ft_incorrect_results_prompt, \"Fine-tuned\", \"Incorrect Bias Prompt\")\n",
    "df_ft_correct_prompt = prepare_data_for_plotting(ft_correct_results_prompt, \"Fine-tuned\", \"Correct Bias Prompt\")\n",
    "\n",
    "# Combine all conditions into one DataFrame\n",
    "df_combined = pd.concat([\n",
    "    df_base_incorrect, df_base_correct,\n",
    "    df_base_incorrect_prompt, df_base_correct_prompt,\n",
    "    df_ft_incorrect, df_ft_correct,\n",
    "    df_ft_incorrect_prompt, df_ft_correct_prompt\n",
    "], ignore_index=True)\n",
    "\n",
    "#-----------------------------\n",
    "# Compute Unbiased Accuracies\n",
    "#-----------------------------\n",
    "\n",
    "# Original unbiased\n",
    "base_unbiased_correct = sum(1 for entry in orig_unbiased_results\n",
    "                            if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "base_unbiased_total = len(orig_unbiased_results)\n",
    "base_unbiased_accuracy = base_unbiased_correct / base_unbiased_total if base_unbiased_total > 0 else 0\n",
    "\n",
    "ft_unbiased_correct = sum(1 for entry in ft_unbiased_results\n",
    "                          if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "ft_unbiased_total = len(ft_unbiased_results)\n",
    "ft_unbiased_accuracy = ft_unbiased_correct / ft_unbiased_total if ft_unbiased_total > 0 else 0\n",
    "\n",
    "# Unbiased prompt\n",
    "base_unbiased_correct_prompt = sum(1 for entry in orig_unbiased_results_prompt\n",
    "                                   if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "base_unbiased_total_prompt = len(orig_unbiased_results_prompt)\n",
    "base_unbiased_accuracy_prompt = base_unbiased_correct_prompt / base_unbiased_total_prompt if base_unbiased_total_prompt > 0 else 0\n",
    "\n",
    "ft_unbiased_correct_prompt = sum(1 for entry in ft_unbiased_results_prompt\n",
    "                                 if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "ft_unbiased_total_prompt = len(ft_unbiased_results_prompt)\n",
    "ft_unbiased_accuracy_prompt = ft_unbiased_correct_prompt / ft_unbiased_total_prompt if ft_unbiased_total_prompt > 0 else 0\n",
    "\n",
    "#-----------------------------\n",
    "# Compute Accuracy per Bias/Model/Dataset\n",
    "#-----------------------------\n",
    "accuracy_df = df_combined.groupby(['Model', 'Dataset', 'Bias'])['Is Correct'].mean().reset_index(name='Accuracy')\n",
    "\n",
    "# Create a \"Condition\" column that combines Model and Dataset for plotting\n",
    "accuracy_df['Condition'] = accuracy_df['Model'] + \" - \" + accuracy_df['Dataset']\n",
    "\n",
    "#-----------------------------\n",
    "# Plotting\n",
    "#-----------------------------\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Adjust hue_order to place base conditions before fine-tuned\n",
    "hue_order = [\n",
    "    \"Base - Correct Bias\", \n",
    "    \"Base - Incorrect Bias\", \n",
    "    \"Base - Correct Bias Prompt\",\n",
    "    \"Base - Incorrect Bias Prompt\",\n",
    "    \"Fine-tuned - Correct Bias\", \n",
    "    \"Fine-tuned - Incorrect Bias\",\n",
    "    \"Fine-tuned - Correct Bias Prompt\",\n",
    "    \"Fine-tuned - Incorrect Bias Prompt\"\n",
    "]\n",
    "\n",
    "sns.barplot(\n",
    "    data=accuracy_df, \n",
    "    x='Bias', \n",
    "    y='Accuracy', \n",
    "    hue='Condition', \n",
    "    palette='Paired', \n",
    "    order=biases, \n",
    "    hue_order=hue_order\n",
    ")\n",
    "\n",
    "plt.title('Accuracy per Cognitive Bias (Base vs. Fine-tuned Models, With and Without Bias Mitigation Prompt)', fontsize=15)\n",
    "plt.xlabel('Bias', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add lines for unbiased accuracies\n",
    "plt.axhline(y=base_unbiased_accuracy, color='blue', linestyle='--', linewidth=2)\n",
    "plt.axhline(y=ft_unbiased_accuracy, color='red', linestyle='--', linewidth=2)\n",
    "plt.axhline(y=base_unbiased_accuracy_prompt, color='green', linestyle='--', linewidth=2)\n",
    "plt.axhline(y=ft_unbiased_accuracy_prompt, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# Create custom legend handles for the unbiased lines\n",
    "line_base_unbiased = Line2D([0], [0], color='blue', linestyle='--', label=f'Base Unbiased ({base_unbiased_accuracy:.2%})')\n",
    "line_ft_unbiased = Line2D([0], [0], color='red', linestyle='--', label=f'Fine-tuned Unbiased ({ft_unbiased_accuracy:.2%})')\n",
    "line_base_unbiased_prompt = Line2D([0], [0], color='green', linestyle='--', label=f'Base Unbiased (w/ Prompt) ({base_unbiased_accuracy_prompt:.2%})')\n",
    "line_ft_unbiased_prompt = Line2D([0], [0], color='orange', linestyle='--', label=f'Fine-tuned Unbiased (w/ Prompt) ({ft_unbiased_accuracy_prompt:.2%})')\n",
    "\n",
    "# Get existing handles and labels from the plot\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# Combine the existing legend with the new lines\n",
    "legend_handles = handles + [line_base_unbiased, line_base_unbiased_prompt, line_ft_unbiased, line_ft_unbiased_prompt]\n",
    "legend_labels = labels + [\n",
    "    f'Base Unbiased ({base_unbiased_accuracy:.2%})',\n",
    "    f'Base Unbiased (w/ Prompt) ({base_unbiased_accuracy_prompt:.2%})',\n",
    "    f'Fine-tuned Unbiased ({ft_unbiased_accuracy:.2%})',\n",
    "    f'Fine-tuned Unbiased (w/ Prompt) ({ft_unbiased_accuracy_prompt:.2%})'\n",
    "]\n",
    "\n",
    "plt.legend(\n",
    "    handles=legend_handles,\n",
    "    labels=legend_labels,\n",
    "    title='Condition',\n",
    "    fontsize=14,\n",
    "    title_fontsize=14,\n",
    "    loc='lower center',  # Place legend below the plot\n",
    "    ncol=3,\n",
    "    bbox_to_anchor=(0.5, -1.03)  # Adjust position as needed\n",
    ")\n",
    "\n",
    "\n",
    "plt.savefig('all_conditions_accuracy_comparison_with_prompt.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b929ba4-7cfc-4496-9271-4d6bd7bbed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# List of biases in a fixed order for plotting\n",
    "biases = [\n",
    "    \"self_diagnosis\", \"recency\", \"confirmation\", \"confirmation_v2\",\n",
    "    \"frequency\", \"cultural\", \"status_quo\", \"false_consensus\", \"false_consensus_v2\"\n",
    "]\n",
    "\n",
    "#-----------------------------\n",
    "# Load All Results Files\n",
    "#-----------------------------\n",
    "\n",
    "# Original, base model, biased scenarios\n",
    "with open(\"cogbias_incorrect_results_with_bias.json\", \"r\") as f:\n",
    "    orig_incorrect_results = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias.json\", \"r\") as f:\n",
    "    orig_correct_results = json.load(f)\n",
    "\n",
    "# Fine-tuned, biased scenarios\n",
    "with open(\"cogbias_incorrect_results_with_bias_finetuned_round2.json\", \"r\") as f:\n",
    "    ft_incorrect_results = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias_finetuned_round2.json\", \"r\") as f:\n",
    "    ft_correct_results = json.load(f)\n",
    "\n",
    "# Original, base model, biased scenarios with prompt\n",
    "with open(\"cogbias_incorrect_results_with_bias_prompt.json\", \"r\") as f:\n",
    "    orig_incorrect_results_prompt = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias_prompt.json\", \"r\") as f:\n",
    "    orig_correct_results_prompt = json.load(f)\n",
    "\n",
    "# Fine-tuned, biased scenarios with prompt\n",
    "with open(\"cogbias_incorrect_results_with_bias_finetuned_round2_prompt.json\", \"r\") as f:\n",
    "    ft_incorrect_results_prompt = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias_finetuned_round2_prompt.json\", \"r\") as f:\n",
    "    ft_correct_results_prompt = json.load(f)\n",
    "\n",
    "# Unbiased (original and fine-tuned)\n",
    "with open(\"results/unbiased_base.json\", \"r\") as f:\n",
    "    orig_unbiased_results = json.load(f)\n",
    "with open(\"results/unbiased_finetuned.json\", \"r\") as f:\n",
    "    ft_unbiased_results = json.load(f)\n",
    "\n",
    "# Unbiased with prompt (original and fine-tuned)\n",
    "with open(\"results/unbiased_base_prompt.json\", \"r\") as f:\n",
    "    orig_unbiased_results_prompt = json.load(f)\n",
    "with open(\"results/unbiased_finetuned_prompt.json\", \"r\") as f:\n",
    "    ft_unbiased_results_prompt = json.load(f)\n",
    "\n",
    "#-----------------------------\n",
    "# Helper Functions\n",
    "#-----------------------------\n",
    "\n",
    "def prepare_biased_data(results, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Prepares biased data for plotting.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of result entries.\n",
    "        model_name (str): 'Base' or 'Fine-tuned'.\n",
    "        dataset_name (str): e.g., 'Correct Bias', 'Incorrect Bias', etc.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with Bias, Is Correct, Model, Dataset.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for entry in results:\n",
    "        correct_answer = entry.get('correct_answer', '').strip()\n",
    "        response = entry.get('response', '').strip()\n",
    "        bias = entry.get('bias', 'unknown')\n",
    "        is_correct = (correct_answer == response)\n",
    "        data.append({\n",
    "            'Bias': bias,\n",
    "            'Is Correct': is_correct,\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset_name\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def prepare_unbiased_data(unbiased_results, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Prepares unbiased data for merging.\n",
    "\n",
    "    Args:\n",
    "        unbiased_results (list): List of unbiased result entries.\n",
    "        model_name (str): 'Base' or 'Fine-tuned'.\n",
    "        dataset_name (str): e.g., 'Correct Bias', 'Incorrect Bias', etc.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with Model, Dataset, Is Correct.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for entry in unbiased_results:\n",
    "        correct_answer = entry.get('correct_answer', '').strip()\n",
    "        response = entry.get('response', '').strip()\n",
    "        is_correct = (correct_answer == response)\n",
    "        data.append({\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset_name,\n",
    "            'Unbiased_Is_Correct': is_correct\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "#-----------------------------\n",
    "# Prepare Biased DataFrames\n",
    "#-----------------------------\n",
    "\n",
    "# Without prompt\n",
    "df_base_incorrect = prepare_biased_data(orig_incorrect_results, \"Base\", \"Incorrect Bias\")\n",
    "df_base_correct = prepare_biased_data(orig_correct_results, \"Base\", \"Correct Bias\")\n",
    "df_ft_incorrect = prepare_biased_data(ft_incorrect_results, \"Fine-tuned\", \"Incorrect Bias\")\n",
    "df_ft_correct = prepare_biased_data(ft_correct_results, \"Fine-tuned\", \"Correct Bias\")\n",
    "\n",
    "# With prompt\n",
    "df_base_incorrect_prompt = prepare_biased_data(orig_incorrect_results_prompt, \"Base\", \"Incorrect Bias (w/ Prompt)\")\n",
    "df_base_correct_prompt = prepare_biased_data(orig_correct_results_prompt, \"Base\", \"Correct Bias (w/ Prompt)\")\n",
    "df_ft_incorrect_prompt = prepare_biased_data(ft_incorrect_results_prompt, \"Fine-tuned\", \"Incorrect Bias (w/ Prompt)\")\n",
    "df_ft_correct_prompt = prepare_biased_data(ft_correct_results_prompt, \"Fine-tuned\", \"Correct Bias (w/ Prompt)\")\n",
    "\n",
    "# Combine all biased data into one DataFrame\n",
    "df_combined = pd.concat([\n",
    "    df_base_incorrect, df_base_correct,\n",
    "    df_base_incorrect_prompt, df_base_correct_prompt,\n",
    "    df_ft_incorrect, df_ft_correct,\n",
    "    df_ft_incorrect_prompt, df_ft_correct_prompt\n",
    "], ignore_index=True)\n",
    "\n",
    "# Assign QuestionID based on the order within each Model and Dataset\n",
    "# This assumes that the order of biased entries corresponds to the order of unbiased entries\n",
    "df_combined['QuestionID'] = df_combined.groupby(['Model', 'Dataset']).cumcount()\n",
    "\n",
    "#-----------------------------\n",
    "# Prepare Unbiased DataFrames\n",
    "#-----------------------------\n",
    "\n",
    "# Original unbiased\n",
    "df_unbiased_orig = prepare_unbiased_data(orig_unbiased_results, \"Base\", \"Incorrect Bias\")\n",
    "# Note: Adjust 'Dataset' to match how biased data is labeled\n",
    "df_unbiased_orig_correct = prepare_unbiased_data(orig_unbiased_results, \"Base\", \"Correct Bias\")\n",
    "\n",
    "# Fine-tuned unbiased\n",
    "df_unbiased_ft = prepare_unbiased_data(ft_unbiased_results, \"Fine-tuned\", \"Incorrect Bias\")\n",
    "df_unbiased_ft_correct = prepare_unbiased_data(ft_unbiased_results, \"Fine-tuned\", \"Correct Bias\")\n",
    "\n",
    "# Original unbiased with prompt\n",
    "df_unbiased_orig_prompt = prepare_unbiased_data(orig_unbiased_results_prompt, \"Base\", \"Incorrect Bias (w/ Prompt)\")\n",
    "df_unbiased_orig_correct_prompt = prepare_unbiased_data(orig_unbiased_results_prompt, \"Base\", \"Correct Bias (w/ Prompt)\")\n",
    "\n",
    "# Fine-tuned unbiased with prompt\n",
    "df_unbiased_ft_prompt = prepare_unbiased_data(ft_unbiased_results_prompt, \"Fine-tuned\", \"Incorrect Bias (w/ Prompt)\")\n",
    "df_unbiased_ft_correct_prompt = prepare_unbiased_data(ft_unbiased_results_prompt, \"Fine-tuned\", \"Correct Bias (w/ Prompt)\")\n",
    "\n",
    "# Combine all unbiased data into one DataFrame\n",
    "df_unbiased_combined = pd.concat([\n",
    "    df_unbiased_orig, df_unbiased_orig_correct,\n",
    "    df_unbiased_orig_prompt, df_unbiased_orig_correct_prompt,\n",
    "    df_unbiased_ft, df_unbiased_ft_correct,\n",
    "    df_unbiased_ft_prompt, df_unbiased_ft_correct_prompt\n",
    "], ignore_index=True)\n",
    "\n",
    "# Assign QuestionID based on the order within each Model and Dataset\n",
    "df_unbiased_combined['QuestionID'] = df_unbiased_combined.groupby(['Model', 'Dataset']).cumcount()\n",
    "\n",
    "#-----------------------------\n",
    "# Repeat Unbiased Data 9 Times (for each bias)\n",
    "#-----------------------------\n",
    "\n",
    "# Since there are 9 biases, each unbiased entry needs to be repeated 9 times to match the biased entries\n",
    "df_unbiased_repeated = df_unbiased_combined.loc[df_unbiased_combined.index.repeat(9)].reset_index(drop=True)\n",
    "\n",
    "#-----------------------------\n",
    "# Merge Biased and Unbiased Data\n",
    "#-----------------------------\n",
    "\n",
    "# Ensure that both DataFrames have the same number of rows after repetition\n",
    "assert len(df_combined) == len(df_unbiased_repeated), \"Mismatch in the number of rows between biased and unbiased data.\"\n",
    "\n",
    "# Merge the biased data with the repeated unbiased data based on Model, Dataset, and QuestionID\n",
    "df_merged = pd.concat([df_combined, df_unbiased_repeated[['Unbiased_Is_Correct']]], axis=1)\n",
    "\n",
    "#-----------------------------\n",
    "# Calculate Percent Matching\n",
    "#-----------------------------\n",
    "\n",
    "# Create a column that indicates if the biased entry matches the unbiased correctness\n",
    "df_merged['Is_Matching'] = df_merged['Is Correct'] == df_merged['Unbiased_Is_Correct']\n",
    "\n",
    "# Group by Model, Dataset, and Bias to calculate the percent matching\n",
    "accuracy_df = df_merged.groupby(['Model', 'Dataset', 'Bias'])['Is_Matching'].mean().reset_index()\n",
    "accuracy_df.rename(columns={'Is_Matching': 'Percent_Matching'}, inplace=True)\n",
    "\n",
    "# Create a \"Condition\" column that combines Model and Dataset for plotting\n",
    "accuracy_df['Condition'] = accuracy_df['Model'] + \" - \" + accuracy_df['Dataset']\n",
    "\n",
    "#-----------------------------\n",
    "# Plotting\n",
    "#-----------------------------\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define the order of hue for consistency\n",
    "hue_order = [\n",
    "    \"Base - Correct Bias\", \n",
    "    \"Base - Incorrect Bias\", \n",
    "    \"Base - Correct Bias (w/ Prompt)\",\n",
    "    \"Base - Incorrect Bias (w/ Prompt)\",\n",
    "    \"Fine-tuned - Correct Bias\", \n",
    "    \"Fine-tuned - Incorrect Bias\",\n",
    "    \"Fine-tuned - Correct Bias (w/ Prompt)\",\n",
    "    \"Fine-tuned - Incorrect Bias (w/ Prompt)\"\n",
    "]\n",
    "\n",
    "sns.barplot(\n",
    "    data=accuracy_df, \n",
    "    x='Bias', \n",
    "    y='Percent_Matching', \n",
    "    hue='Condition', \n",
    "    palette='Paired', \n",
    "    order=biases, \n",
    "    hue_order=hue_order\n",
    ")\n",
    "\n",
    "plt.title('Percent Matching with Unbiased Results per Cognitive Bias\\n(Base vs. Fine-tuned Models, With and Without Bias Mitigation Prompt)', fontsize=15)\n",
    "plt.xlabel('Cognitive Bias', fontsize=14)\n",
    "plt.ylabel('Proportion Matching', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Update the legend\n",
    "plt.legend(\n",
    "    title='Condition', \n",
    "    fontsize=14, \n",
    "    title_fontsize=14,\n",
    "    ncol=3,\n",
    "    loc='lower center',  # Place legend below the plot\n",
    "    bbox_to_anchor=(0.5, -1)  # Adjust position as needed\n",
    ")\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('all_conditions_percent_matching_comparison_with_prompt.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331578c0-45a4-4c64-9433-63b62c570329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# List of biases in a fixed order for plotting\n",
    "biases = [\"self_diagnosis\", \"recency\", \"confirmation\", \"confirmation_v2\",\n",
    "          \"frequency\", \"cultural\", \"status_quo\", \"false_consensus\", \"false_consensus_v2\"]\n",
    "\n",
    "#-----------------------------\n",
    "# Load All Results Files\n",
    "#-----------------------------\n",
    "\n",
    "# Original, base model, biased scenarios\n",
    "with open(\"cogbias_incorrect_results_with_bias.json\", \"r\") as f:\n",
    "    orig_incorrect_results = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias.json\", \"r\") as f:\n",
    "    orig_correct_results = json.load(f)\n",
    "\n",
    "# Fine-tuned, biased scenarios\n",
    "with open(\"cogbias_incorrect_results_with_bias_finetuned_round2.json\", \"r\") as f:\n",
    "    ft_incorrect_results = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias_finetuned_round2.json\", \"r\") as f:\n",
    "    ft_correct_results = json.load(f)\n",
    "\n",
    "# Original, base model, biased scenarios with prompt\n",
    "with open(\"cogbias_incorrect_results_with_bias_prompt.json\", \"r\") as f:\n",
    "    orig_incorrect_results_prompt = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias_prompt.json\", \"r\") as f:\n",
    "    orig_correct_results_prompt = json.load(f)\n",
    "\n",
    "# Fine-tuned, biased scenarios with prompt -- INCLUDED BUT COMMENTED OUT\n",
    "with open(\"cogbias_incorrect_results_with_bias_finetuned_round2_prompt.json\", \"r\") as f:\n",
    "    ft_incorrect_results_prompt = json.load(f)\n",
    "with open(\"cogbias_correct_results_with_bias_finetuned_round2_prompt.json\", \"r\") as f:\n",
    "    ft_correct_results_prompt = json.load(f)\n",
    "\n",
    "# Unbiased (original and fine-tuned)\n",
    "with open(\"results/unbiased_base.json\", \"r\") as f:\n",
    "    orig_unbiased_results = json.load(f)\n",
    "with open(\"results/unbiased_finetuned.json\", \"r\") as f:\n",
    "    ft_unbiased_results = json.load(f)\n",
    "\n",
    "# Unbiased with prompt (original and fine-tuned)\n",
    "with open(\"results/unbiased_base_prompt.json\", \"r\") as f:\n",
    "    orig_unbiased_results_prompt = json.load(f)\n",
    "with open(\"results/unbiased_finetuned_prompt.json\", \"r\") as f:\n",
    "    ft_unbiased_results_prompt = json.load(f)\n",
    "#-----------------------------\n",
    "# Helper Function\n",
    "#-----------------------------\n",
    "def prepare_data_for_plotting(results, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    results: a list of entries, each with 'correct_answer', 'response', and 'bias' fields\n",
    "    model_name: 'Base' or 'Fine-tuned'\n",
    "    dataset_name: Something like 'Correct Bias', 'Incorrect Bias', 'Correct Bias Prompt', etc.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for entry in results:\n",
    "        correct_answer = entry.get('correct_answer', '').strip()\n",
    "        response = entry.get('response', '').strip()\n",
    "        bias = entry.get('bias', 'unknown')\n",
    "        is_correct = (correct_answer == response)\n",
    "        data.append({'Bias': bias, 'Is Correct': is_correct, 'Model': model_name, 'Dataset': dataset_name})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "#-----------------------------\n",
    "# Prepare DataFrames\n",
    "#-----------------------------\n",
    "\n",
    "# Without prompt\n",
    "df_base_incorrect = prepare_data_for_plotting(orig_incorrect_results, \"Base\", \"Incorrect Bias\")\n",
    "df_base_correct = prepare_data_for_plotting(orig_correct_results, \"Base\", \"Correct Bias\")\n",
    "df_ft_incorrect = prepare_data_for_plotting(ft_incorrect_results, \"Fine-tuned\", \"Incorrect Bias\")\n",
    "df_ft_correct = prepare_data_for_plotting(ft_correct_results, \"Fine-tuned\", \"Correct Bias\")\n",
    "\n",
    "# With prompt\n",
    "df_base_incorrect_prompt = prepare_data_for_plotting(orig_incorrect_results_prompt, \"Base\", \"Incorrect Bias Prompt\")\n",
    "df_base_correct_prompt = prepare_data_for_plotting(orig_correct_results_prompt, \"Base\", \"Correct Bias Prompt\")\n",
    "\n",
    "# Fine-tuned with prompt DataFrames\n",
    "df_ft_incorrect_prompt = prepare_data_for_plotting(ft_incorrect_results_prompt, \"Fine-tuned\", \"Incorrect Bias Prompt\")\n",
    "df_ft_correct_prompt = prepare_data_for_plotting(ft_correct_results_prompt, \"Fine-tuned\", \"Correct Bias Prompt\")\n",
    "\n",
    "# Combine all conditions into one DataFrame\n",
    "df_combined = pd.concat([\n",
    "    df_base_incorrect, df_base_correct,\n",
    "    df_base_incorrect_prompt, df_base_correct_prompt,\n",
    "    df_ft_incorrect, df_ft_correct,\n",
    "    df_ft_incorrect_prompt, df_ft_correct_prompt\n",
    "], ignore_index=True)\n",
    "\n",
    "#-----------------------------\n",
    "# Compute Unbiased Accuracies\n",
    "#-----------------------------\n",
    "\n",
    "# Original unbiased\n",
    "base_unbiased_correct = sum(1 for entry in orig_unbiased_results\n",
    "                            if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "base_unbiased_total = len(orig_unbiased_results)\n",
    "base_unbiased_accuracy = base_unbiased_correct / base_unbiased_total if base_unbiased_total > 0 else 0\n",
    "\n",
    "ft_unbiased_correct = sum(1 for entry in ft_unbiased_results\n",
    "                          if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "ft_unbiased_total = len(ft_unbiased_results)\n",
    "ft_unbiased_accuracy = ft_unbiased_correct / ft_unbiased_total if ft_unbiased_total > 0 else 0\n",
    "\n",
    "# Unbiased prompt\n",
    "base_unbiased_correct_prompt = sum(1 for entry in orig_unbiased_results_prompt\n",
    "                                   if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "base_unbiased_total_prompt = len(orig_unbiased_results_prompt)\n",
    "base_unbiased_accuracy_prompt = base_unbiased_correct_prompt / base_unbiased_total_prompt if base_unbiased_total_prompt > 0 else 0\n",
    "\n",
    "ft_unbiased_correct_prompt = sum(1 for entry in ft_unbiased_results_prompt\n",
    "                                 if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "ft_unbiased_total_prompt = len(ft_unbiased_results_prompt)\n",
    "ft_unbiased_accuracy_prompt = ft_unbiased_correct_prompt / ft_unbiased_total_prompt if ft_unbiased_total_prompt > 0 else 0\n",
    "\n",
    "#-----------------------------\n",
    "# Compute Accuracy per Bias/Model/Dataset\n",
    "#-----------------------------\n",
    "accuracy_df = df_combined.groupby(['Model', 'Dataset', 'Bias'])['Is Correct'].mean().reset_index(name='Accuracy')\n",
    "\n",
    "# Create a \"Condition\" column that combines Model and Dataset for plotting\n",
    "accuracy_df['Condition'] = accuracy_df['Model'] + \" - \" + accuracy_df['Dataset']\n",
    "\n",
    "#-----------------------------\n",
    "# Define Custom Palette\n",
    "#-----------------------------\n",
    "# custom_palette = [\n",
    "#     '#ADD8E6',  # Base - Correct Bias: Light Blue\n",
    "#     '#F08080',  # Base - Incorrect Bias: Light Coral\n",
    "#     '#90EE90',  # Base - Correct Bias Prompt: Light Green\n",
    "#     '#FFD700',  # Base - Incorrect Bias Prompt: Gold\n",
    "#     '#00008B',  # Fine-tuned - Correct Bias: Dark Blue\n",
    "#     '#8B0000',  # Fine-tuned - Incorrect Bias: Dark Red\n",
    "#     '#006400',  # Fine-tuned - Correct Bias Prompt: Dark Green\n",
    "#     '#FF8C00'   # Fine-tuned - Incorrect Bias Prompt: Dark Orange\n",
    "# ]\n",
    "\n",
    "paired_palette = sns.color_palette(\"Paired\", 8)\n",
    "\n",
    "# Define the custom_palette with the first 8 colors\n",
    "custom_palette = [\n",
    "    paired_palette[0],  # Base - Correct Bias: #a6cee3\n",
    "    paired_palette[2],  # Fine-tuned - Correct Bias: #1f78b4\n",
    "    paired_palette[4],  # Base - Incorrect Bias: #b2df8a\n",
    "    paired_palette[6],  # Fine-tuned - Incorrect Bias: #33a02c\n",
    "    paired_palette[1],  # Base - Correct Bias Prompt: #fb9a99\n",
    "    paired_palette[3],  # Fine-tuned - Correct Bias Prompt: #e31a1c\n",
    "    paired_palette[5],  # Base - Incorrect Bias Prompt: #fdbf6f\n",
    "    paired_palette[7]   # Fine-tuned - Incorrect Bias Prompt: #ff7f00\n",
    "]\n",
    "\n",
    "#-----------------------------\n",
    "# Plotting\n",
    "#-----------------------------\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Adjust hue_order to place base conditions before fine-tuned\n",
    "hue_order = [\n",
    "    \"Base - Correct Bias\", \n",
    "    \"Base - Incorrect Bias\", \n",
    "    \"Base - Correct Bias Prompt\",\n",
    "    \"Base - Incorrect Bias Prompt\",\n",
    "    \"Fine-tuned - Correct Bias\", \n",
    "    \"Fine-tuned - Incorrect Bias\",\n",
    "    \"Fine-tuned - Correct Bias Prompt\",\n",
    "    \"Fine-tuned - Incorrect Bias Prompt\"\n",
    "]\n",
    "\n",
    "sns.barplot(\n",
    "    data=accuracy_df, \n",
    "    x='Bias', \n",
    "    y='Accuracy', \n",
    "    hue='Condition', \n",
    "    palette=custom_palette,  # Use the custom palette here\n",
    "    order=biases, \n",
    "    hue_order=hue_order\n",
    ")\n",
    "\n",
    "plt.title('Accuracy per Cognitive Bias (Base vs. Fine-tuned Models, With and Without Bias Mitigation Prompt)', fontsize=18)\n",
    "plt.xlabel('Bias', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add lines for unbiased accuracies\n",
    "plt.axhline(y=base_unbiased_accuracy, color='blue', linestyle='--', linewidth=2)\n",
    "plt.axhline(y=ft_unbiased_accuracy, color='red', linestyle='--', linewidth=2)\n",
    "plt.axhline(y=base_unbiased_accuracy_prompt, color='green', linestyle='--', linewidth=2)\n",
    "plt.axhline(y=ft_unbiased_accuracy_prompt, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# Create custom legend handles for the unbiased lines\n",
    "line_base_unbiased = Line2D([0], [0], color='blue', linestyle='--', label=f'Base Unbiased ({base_unbiased_accuracy:.2%})')\n",
    "line_ft_unbiased = Line2D([0], [0], color='red', linestyle='--', label=f'Fine-tuned Unbiased ({ft_unbiased_accuracy:.2%})')\n",
    "line_base_unbiased_prompt = Line2D([0], [0], color='green', linestyle='--', label=f'Base Unbiased (w/ Prompt) ({base_unbiased_accuracy_prompt:.2%})')\n",
    "line_ft_unbiased_prompt = Line2D([0], [0], color='orange', linestyle='--', label=f'Fine-tuned Unbiased (w/ Prompt) ({ft_unbiased_accuracy_prompt:.2%})')\n",
    "\n",
    "# Get existing handles and labels from the plot\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# Combine the existing legend with the new lines\n",
    "legend_handles = handles + [line_base_unbiased, line_base_unbiased_prompt, line_ft_unbiased, line_ft_unbiased_prompt]\n",
    "legend_labels = labels + [\n",
    "    f'Base Unbiased ({base_unbiased_accuracy:.2%})',\n",
    "    f'Base Unbiased (w/ Prompt) ({base_unbiased_accuracy_prompt:.2%})',\n",
    "    f'Fine-tuned Unbiased ({ft_unbiased_accuracy:.2%})',\n",
    "    f'Fine-tuned Unbiased (w/ Prompt) ({ft_unbiased_accuracy_prompt:.2%})'\n",
    "]\n",
    "\n",
    "plt.legend(\n",
    "    handles=legend_handles,\n",
    "    labels=legend_labels,\n",
    "    title='Condition', \n",
    "    fontsize=12, \n",
    "    title_fontsize=14,\n",
    "    bbox_to_anchor=(1.05, 1), \n",
    "    loc='upper left'\n",
    ")\n",
    "\n",
    "plt.savefig('all_conditions_accuracy_comparison_with_prompt_reordered.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481dff8-4a85-4f74-9c82-f323b4297fe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Updated Data with Bias Mitigation\n",
    "data = {\n",
    "  'Dataset': [\n",
    "      'JAMA',\n",
    "      'JAMA',\n",
    "      'JAMA',\n",
    "      'JAMA',\n",
    "  ],\n",
    "  'Model': [\n",
    "      'Base',\n",
    "      'Base',\n",
    "      'Fine-tuned',\n",
    "      'Fine-tuned'\n",
    "  ],\n",
    "  'Bias Mitigation': [\n",
    "      'No',\n",
    "      'Yes',\n",
    "      'No',\n",
    "      'Yes'\n",
    "  ],\n",
    "  'Correct': [\n",
    "      761, 762, 796, 791,  # Base without bias mitigation\n",
    "      # Base with bias mitigation\n",
    "      # Fine-tuned without bias mitigation\n",
    "      # Fine-tuned with bias mitigation\n",
    "  ],\n",
    "  'Total': [\n",
    "      1524, 1524, 1524, 1524,  # Base without bias mitigation\n",
    "      # Base with bias mitigation\n",
    "      # Fine-tuned without bias mitigation\n",
    "      # Fine-tuned with bias mitigation\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute Accuracy\n",
    "df['Accuracy'] = df['Correct'] / df['Total']\n",
    "\n",
    "# Combine 'Model' and 'Bias Mitigation' for clearer legend labels\n",
    "df['Model + Bias Mitigation'] = df.apply(\n",
    "  lambda row: f\"{row['Model']} {'(w/ Prompt)' if row['Bias Mitigation'] == 'Yes' else ''}\",\n",
    "  axis=1\n",
    ")\n",
    "\n",
    "# Clean up any extra spaces\n",
    "df['Model + Bias Mitigation'] = df['Model + Bias Mitigation'].str.strip()\n",
    "\n",
    "# Display the DataFrame (Optional)\n",
    "# print(df)\n",
    "\n",
    "# Set Seaborn Style\n",
    "sns.set_style(\"darkgrid\")  # Use darkgrid style\n",
    "\n",
    "# Initialize the Figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the Bar Plot\n",
    "ax = sns.barplot(\n",
    "  x='Dataset',\n",
    "  y='Accuracy',\n",
    "  hue='Model + Bias Mitigation',\n",
    "  data=df[df['Dataset'] == 'JAMA'],  # Filter data for JAMA only\n",
    "  palette='Set2',  # Use built-in color palette 'Set2'\n",
    "  edgecolor='black'\n",
    ")\n",
    "\n",
    "# Add Accuracy Labels on Top of Bars\n",
    "for bar in ax.patches:\n",
    "  height = bar.get_height()\n",
    "  if height > 0:\n",
    "    ax.annotate(\n",
    "      f'{height:.2%}',\n",
    "      xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "      xytext=(0, 5),  # 5 points vertical offset\n",
    "      textcoords=\"offset points\",\n",
    "      ha='center',\n",
    "      va='bottom',\n",
    "      fontsize=9\n",
    "    )\n",
    "\n",
    "# Customize X-axis Labels\n",
    "plt.xticks(rotation=0, ha='center')\n",
    "\n",
    "# Set Labels and Title\n",
    "plt.xlabel('Dataset', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Accuracy for JAMA Dataset: Base vs Fine-tuned Models', fontsize=14)\n",
    "\n",
    "# Set Y-axis Limits\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Customize Legend\n",
    "plt.legend(title='Model', loc='upper left')\n",
    "\n",
    "# Optimize Layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the Plot as PNG (Optional)\n",
    "# plt.savefig('model_comparison_jama.png', dpi=300)\n",
    "\n",
    "# Display the Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b4ae0-1fbe-4c36-a5eb-96eb59f87969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for JAMA\n",
    "jama_data = [\n",
    "    (\"Base\", 761 / 1524),\n",
    "    (\"Base with Prompt\", 762 / 1524),\n",
    "    (\"Fine-tuned\", 796 / 1524),\n",
    "    (\"Fine-tuned with Prompt\", 791 / 1524)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(jama_data, columns=[\"Model\", \"Accuracy\"])\n",
    "\n",
    "# Set Seaborn Style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Initialize the Figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the Bar Plot\n",
    "ax = sns.barplot(\n",
    "    x=\"Model\",\n",
    "    y=\"Accuracy\",\n",
    "    data=df,\n",
    "    palette=\"Set2\",\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "# Add Accuracy Labels on Top of Bars\n",
    "for bar in ax.patches:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(\n",
    "        f\"{height:.2%}\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "        xytext=(0, 5),  # 5 points vertical offset\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=14\n",
    "    )\n",
    "\n",
    "# Customize X-axis Labels\n",
    "plt.xticks(rotation=0, ha='center', fontsize=14)\n",
    "\n",
    "# Set Labels and Title\n",
    "plt.xlabel(\"Model\", fontsize=0)\n",
    "plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "plt.title(\"Accuracy for JAMA Dataset\", fontsize=15)\n",
    "\n",
    "# Set Y-axis Limits\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Optimize Layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('model_comparison_jama.png', dpi=300)\n",
    "\n",
    "\n",
    "# Display the Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80066cfd-5228-4df1-a695-b8403b3abd73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# List of biases in order\n",
    "biases = [\"self_diagnosis\", \"recency\", \"confirmation\", \"confirmation_v2\",\n",
    "          \"frequency\", \"cultural\", \"status_quo\", \"false_consensus\", \"false_consensus_v2\"]\n",
    "\n",
    "# Read in all four results files with biases included\n",
    "with open(\"cogbias_incorrect_results_with_bias.json\", \"r\") as f:\n",
    "    orig_incorrect_results = json.load(f)\n",
    "\n",
    "with open(\"cogbias_correct_results_with_bias.json\", \"r\") as f:\n",
    "    orig_correct_results = json.load(f)\n",
    "\n",
    "with open(\"cogbias_incorrect_results_with_bias_finetuned_round2.json\", \"r\") as f:\n",
    "    ft_incorrect_results = json.load(f)\n",
    "\n",
    "with open(\"cogbias_correct_results_with_bias_finetuned_round2.json\", \"r\") as f:\n",
    "    ft_correct_results = json.load(f)\n",
    "\n",
    "# Read in unbiased results for both base and fine-tuned\n",
    "with open(\"results/unbiased_base.json\", \"r\") as f:\n",
    "    orig_unbiased_results = json.load(f)\n",
    "\n",
    "with open(\"results/unbiased_finetuned.json\", \"r\") as f:\n",
    "    ft_unbiased_results = json.load(f)\n",
    "\n",
    "def prepare_data_for_plotting(results, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    results: a list of entries, each with 'correct_answer', 'response', and 'bias' fields\n",
    "    model_name: 'Base' or 'Fine-tuned'\n",
    "    dataset_name: 'Correct Bias' or 'Incorrect Bias'\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for entry in results:\n",
    "        correct_answer = entry.get('correct_answer', '').strip()\n",
    "        response = entry.get('response', '').strip()\n",
    "        bias = entry.get('bias', 'unknown')\n",
    "        is_correct = (correct_answer == response)\n",
    "        data.append({'Bias': bias, 'Is Correct': is_correct, 'Model': model_name, 'Dataset': dataset_name})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Prepare dataframes for each condition\n",
    "df_base_incorrect = prepare_data_for_plotting(orig_incorrect_results, \"Base\", \"Incorrect Bias\")\n",
    "df_base_correct = prepare_data_for_plotting(orig_correct_results, \"Base\", \"Correct Bias\")\n",
    "df_ft_incorrect = prepare_data_for_plotting(ft_incorrect_results, \"Fine-tuned\", \"Incorrect Bias\")\n",
    "df_ft_correct = prepare_data_for_plotting(ft_correct_results, \"Fine-tuned\", \"Correct Bias\")\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "df_combined = pd.concat([df_base_incorrect, df_base_correct, df_ft_incorrect, df_ft_correct], ignore_index=True)\n",
    "\n",
    "# Compute unbiased accuracy for base and fine-tuned\n",
    "base_unbiased_correct = sum(1 for entry in orig_unbiased_results \n",
    "                            if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "base_unbiased_total = len(orig_unbiased_results)\n",
    "base_unbiased_accuracy = base_unbiased_correct / base_unbiased_total if base_unbiased_total > 0 else 0\n",
    "\n",
    "ft_unbiased_correct = sum(1 for entry in ft_unbiased_results \n",
    "                          if entry.get('correct_answer', '').strip() == entry.get('response', '').strip())\n",
    "ft_unbiased_total = len(ft_unbiased_results)\n",
    "ft_unbiased_accuracy = ft_unbiased_correct / ft_unbiased_total if ft_unbiased_total > 0 else 0\n",
    "\n",
    "# Compute accuracy per Bias, per Model, per Dataset (Correct/Incorrect)\n",
    "accuracy_df = df_combined.groupby(['Model', 'Dataset', 'Bias'])['Is Correct'].mean().reset_index(name='Accuracy')\n",
    "\n",
    "# Pivot the data so we get columns for correct and incorrect conditions per model\n",
    "pivot_df = accuracy_df.pivot_table(index=['Model', 'Bias'], columns='Dataset', values='Accuracy').reset_index()\n",
    "\n",
    "# Ensure both \"Correct Bias\" and \"Incorrect Bias\" columns exist, filling missing with 0 if needed\n",
    "pivot_df['Correct Bias'] = pivot_df.get('Correct Bias', 0)\n",
    "pivot_df['Incorrect Bias'] = pivot_df.get('Incorrect Bias', 0)\n",
    "\n",
    "# Compute difference between incorrect and correct for each model/bias\n",
    "pivot_df['Diff'] = pivot_df['Correct Bias'] - pivot_df['Incorrect Bias']\n",
    "\n",
    "# Separate out base and fine-tuned rows\n",
    "base_df = pivot_df[pivot_df['Model'] == 'Base'].rename(columns={'Diff': 'Base_Diff'})\n",
    "ft_df = pivot_df[pivot_df['Model'] == 'Fine-tuned'].rename(columns={'Diff': 'FT_Diff'})\n",
    "\n",
    "# Merge to get both differences side by side\n",
    "merged_df = pd.merge(base_df[['Bias', 'Base_Diff']], ft_df[['Bias', 'FT_Diff']], on='Bias', how='inner')\n",
    "\n",
    "# Compute the difference in differences: (Base difference) - (Fine-tuned difference)\n",
    "merged_df['Reduction_in_Bias'] = merged_df['Base_Diff'] - merged_df['FT_Diff']\n",
    "\n",
    "# Now we have a DataFrame with Bias and Reduction_in_Bias\n",
    "# Plot this as a barplot\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.barplot(data=merged_df, x='Bias', y='Reduction_in_Bias', order=biases, color='steelblue')\n",
    "plt.title('Reduction in Bias After Fine-tuning', fontsize=18)\n",
    "plt.xlabel('Bias', fontsize=14)\n",
    "plt.ylabel('Reduction in Bias (Base Diff - Fine-tuned Diff)', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add horizontal line at 0 for reference\n",
    "plt.axhline(y=0, color='black', linestyle='-')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bias_reduction_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
