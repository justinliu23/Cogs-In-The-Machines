{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "PAD_TOKEN = \"<|pad|>\"\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can easily be adapted for base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../02_LoRA/finetuned_model\", device_map=\"cuda\")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'test': 'all/test-00000-of-00001.parquet', 'validation': 'all/validation-00000-of-00001.parquet', 'dev': 'all/dev-00000-of-00001.parquet', 'auxiliary_train': 'all/auxiliary_train-00000-of-00001.parquet'}\n",
    "mmlu_test_df = pd.read_parquet(\"hf://datasets/cais/mmlu/\" + splits[\"test\"])\n",
    "mmlu_dev_df = pd.read_parquet(\"hf://datasets/cais/mmlu/\" + splits[\"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from https://github.com/hendrycks/test/blob/master/evaluate.py\n",
    "def softmax(x):\n",
    "    z = x - max(x)\n",
    "    numerator = np.exp(z)\n",
    "    denominator = np.sum(numerator)\n",
    "    softmax = numerator/denominator\n",
    "    return softmax\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \" \".join(l)\n",
    "    return s\n",
    "\n",
    "def format_example(df, idx, include_answer = True):\n",
    "    question = df.loc[idx, 'question']\n",
    "    answer = df.loc[idx, 'answer']\n",
    "    choices = df.loc[idx, 'choices']\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question + '\\n' + 'Choices: ' + str(choices)\n",
    "        },\n",
    "    ]\n",
    "    if include_answer:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": answer,\n",
    "            }\n",
    "        )\n",
    "    return messages\n",
    "\n",
    "def gen_prompt(train_df, subject, k=-1):\n",
    "    messages = []\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"The following are multiple choice questions (with answers) about {}. Answer them to the best of your ability by giving the index (starting at 0) of the correct answer in the list of choices.\".format(format_subject(subject))\n",
    "    })\n",
    "    shot = 0\n",
    "    for i in range(len(train_df)):\n",
    "        if train_df.loc[i, 'subject'] == subject:\n",
    "            messages += format_example(train_df, i)\n",
    "            shot += 1\n",
    "        if shot == 5:\n",
    "            break\n",
    "    \n",
    "    return (tokenizer.apply_chat_template(messages, tokenize=False), shot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare subject 5-shot context\n",
    "subjects = mmlu_test_df['subject'].unique()\n",
    "\n",
    "subject_dev_prompts = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    result = gen_prompt(mmlu_dev_df, subject)\n",
    "    if result[1] != 5:\n",
    "        print(\"Failed to 5-shot\", subject)\n",
    "    subject_dev_prompts[subject] = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "answer_strings = [\"0\", \"1\", \"2\", \"3\"]\n",
    "token_ids = [tokenizer.encode(s)[1] for s in answer_strings]\n",
    "\n",
    "model_outputs = []\n",
    "\n",
    "for i in tqdm(range(mmlu_test_df.shape[0])):\n",
    "    test_prompt = tokenizer.apply_chat_template(format_example(mmlu_test_df, i, include_answer = False), tokenize=False)\n",
    "    cutoff_idx = test_prompt.find('<|eot_id|>')\n",
    "    test_prompt = test_prompt[cutoff_idx + len(\"<|eot_id|>\"):]\n",
    "    test_prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    \n",
    "    subject = mmlu_test_df.loc[i, \"subject\"]\n",
    "    correct_answer = mmlu_test_df.loc[i, \"answer\"]\n",
    "    dev_prompt = subject_dev_prompts[subject]\n",
    "\n",
    "    prompt = dev_prompt + test_prompt\n",
    "\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        tokenized_prompt[\"input_ids\"],\n",
    "        max_new_tokens = 1,\n",
    "        do_sample=False,   # Greedy decoding\n",
    "        return_dict_in_generate=True,  # Get additional generation info\n",
    "        output_scores=True,  # Get the scores/logits\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    logits = outputs.scores[0][0] # Logit for generation token\n",
    "\n",
    "    logits = logits.cpu()\n",
    "    \n",
    "    llm_output = np.argmax(np.array([logits[token_ids][i] for i in range(4)]))\n",
    "    model_outputs.append(llm_output)\n",
    "\n",
    "    if llm_output == correct_answer:\n",
    "        correct += 1\n",
    "    total += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If testing base model instead of finetuned model\n",
    "\"\"\" \n",
    "mmlu_test_df['base_answer'] = model_outputs\n",
    "mmlu_test_df.to_csv(\"03_mmlu/mmlu_base.csv\")\n",
    "\"\"\"\n",
    "\n",
    "mmlu_test_df['finetuned_answer'] = model_outputs\n",
    "mmlu_test_df.to_csv(\"03_MMLU/mmlu_finetuned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
