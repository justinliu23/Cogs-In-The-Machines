{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_base = pd.read_csv(\"mmlu_base.csv\")\n",
    "mmlu_finetuned = pd.read_csv(\"mmlu_finetuned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_correct = {}\n",
    "subject_totals = {}\n",
    "for i in range(len(mmlu_base)):\n",
    "    subject = mmlu_base[\"subject\"][i]\n",
    "    subject_totals[subject] = subject_totals.get(subject, 0) + 1\n",
    "    if mmlu_base.loc[i, 'base_answer'] == mmlu_base.loc[i, 'answer']:\n",
    "        base_correct[subject] = base_correct.get(subject, 0) + 1\n",
    "\n",
    "subjects = sorted(list(subject_totals.keys()))\n",
    "base_scores = [base_correct[s] / subject_totals[s] for s in subjects]\n",
    "base_scores = [s * 100 for s in base_scores]\n",
    "base_correct = [base_correct[s] for s in subjects]\n",
    "plt.bar(range(len(subjects)), base_scores, width=0.6, color=\"blue\")\n",
    "plt.title(\"Base Model MMLU Scores\")\n",
    "plt.ylabel(\"Percentage Correct\")\n",
    "plt.xlabel(\"Subjects\")\n",
    "plt.xticks([])\n",
    "plt.ylim(0, 100)\n",
    "plt.savefig(\"mmlu_base.png\", bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_correct = {}\n",
    "subject_totals = {}\n",
    "for i in range(len(mmlu_finetuned)):\n",
    "    subject = mmlu_finetuned[\"subject\"][i]\n",
    "    subject_totals[subject] = subject_totals.get(subject, 0) + 1\n",
    "    if mmlu_finetuned.loc[i, 'finetuned_answer'] == mmlu_finetuned.loc[i, 'answer']:\n",
    "        finetuned_correct[subject] = finetuned_correct.get(subject, 0) + 1\n",
    "\n",
    "subjects = sorted(list(subject_totals.keys()))\n",
    "finetuned_scores = [finetuned_correct[s] / subject_totals[s] for s in subjects]\n",
    "finetuned_scores = [s * 100 for s in finetuned_scores]\n",
    "finetuned_correct = [finetuned_correct[s] for s in subjects]\n",
    "plt.bar(range(len(subjects)), finetuned_scores, width=0.6, color='red')\n",
    "plt.title(\"CogCotroLM MMLU Scores\")\n",
    "plt.ylabel(\"Percentage Correct\")\n",
    "plt.xlabel(\"Subjects\")\n",
    "plt.xticks([])\n",
    "plt.ylim(0, 100)\n",
    "plt.savefig(\"mmlu_ft.png\", bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = np.subtract(finetuned_correct, base_correct)\n",
    "width = 0.4\n",
    "x = np.arange(len(subjects))\n",
    "plt.bar(x - width / 2, differences, width=width, label='CogControLM - Base', color='purple')\n",
    "plt.bar(x + width / 2, [subject_totals[s] for s in subjects], label='Total', width=width, color='grey')\n",
    "plt.title(\"MMLU Correct Answer Differences\")\n",
    "plt.ylabel(\"# of Questions\")\n",
    "plt.xlabel(\"Subjects\")\n",
    "plt.xticks([])\n",
    "plt.legend()\n",
    "plt.savefig(\"mmlu_diff.png\", bbox_inches='tight', dpi=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
