{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from typing import cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "PAD_TOKEN = \"<|pad|>\"\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can easily be adapted for base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../02_LoRA/finetuned_model\", device_map=\"cuda\")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_LABELS = [\"A\", \"B\", \"C\", \"D\"]\n",
    "PROMPT_PREFIX = \"Please choose the correct answer from among the following options: \\n\"\n",
    "PROMPT_SUFFIX = \"Answer by only stating the correct letter. That is, answer only by stating 'A', 'B', 'C', or 'D'. The correct answer is:\"\n",
    "\n",
    "def generate_question_and_answers(example) -> dict:\n",
    "    # Randomly shuffle the order of the choices every time we generate an exaple\n",
    "    choice_indices = [1, 2, 3, 4]\n",
    "    choice_order = random.sample(choice_indices, len(choice_indices))\n",
    "    ans_idx = choice_order.index(4)\n",
    "\n",
    "    ordered_choices = [\n",
    "        example[f\"Incorrect Answer {i}\"] if i != 4 else example[\"Correct Answer\"]\n",
    "        for i in choice_order\n",
    "    ]\n",
    "    ordered_choices = [\n",
    "        f\"({ANSWER_LABELS[i]}) {choice}\" for i, choice in enumerate(ordered_choices)\n",
    "    ]\n",
    "\n",
    "    context = PROMPT_PREFIX + \"\\n\".join(ordered_choices)\n",
    "    question = PROMPT_SUFFIX\n",
    "    answer = ANSWER_LABELS[ans_idx]\n",
    "\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"answer_start\": context.index(answer),\n",
    "        \"answer_end\": context.index(answer) + len(answer),\n",
    "    }\n",
    "\n",
    "\n",
    "def add_position_data(examples: dict) -> dict:\n",
    "    def find_labels(offsets, ans_start, ans_end, sequence_ids):\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offsets[context_start][0] > ans_end or offsets[context_end][1] < ans_start:\n",
    "            return (0, 0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offsets[idx][0] <= ans_start:\n",
    "                idx += 1\n",
    "            start_position = idx - 1\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offsets[idx][1] >= ans_end:\n",
    "                idx -= 1\n",
    "            end_position = idx + 1\n",
    "\n",
    "            return (start_position, end_position)\n",
    "\n",
    "    positions = {\"start_positions\": [], \"end_positions\": []}\n",
    "    offset_mapping = examples.pop(\"offset_mapping\")\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        start, end = find_labels(\n",
    "            offset,\n",
    "            examples[\"answer_start\"][i],\n",
    "            examples[\"answer_end\"][i],\n",
    "            examples[\"sequence_ids\"][i],\n",
    "        )\n",
    "        positions[\"start_positions\"].append(start)\n",
    "        positions[\"end_positions\"].append(end)\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def tokenize_data(examples: dict, tokenizer) -> dict:\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Add sequence_ids since the Dataset will convert `inputs` to a\n",
    "    # dictionary, destroying the `sequence_ids()` method.\n",
    "    inputs[\"sequence_ids\"] = [\n",
    "        inputs.sequence_ids(i) for i in range(inputs[\"offset_mapping\"].shape[0])\n",
    "    ]\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpqa_ds = load_dataset(\"csv\", data_files=\"gpqa_extended.csv\", split=\"all\")\n",
    "gpqa_ds = cast(Dataset, gpqa_ds)\n",
    "\n",
    "gpqa_ds = gpqa_ds.rename_columns({\"Question\": \"Dataset Question\"})\n",
    "\n",
    "gpqa_ds = gpqa_ds.map(generate_question_and_answers, batched=False)\n",
    "gpqa_ds = gpqa_ds.map(lambda examples: tokenize_data(examples, tokenizer), batched=True)\n",
    "gpqa_ds = gpqa_ds.map(lambda example: add_position_data(example), batched=True)\n",
    "\n",
    "cols_to_remove = gpqa_ds.column_names\n",
    "cols_to_remove.remove(\"context\")\n",
    "cols_to_remove.remove(\"question\")\n",
    "cols_to_remove.remove(\"answer\")\n",
    "cols_to_remove.remove(\"Dataset Question\")\n",
    "cols_to_remove.remove(\"Subdomain\")\n",
    "test_data = gpqa_ds.remove_columns(cols_to_remove)\n",
    "\n",
    "test_data.to_csv(\"altered_gpqa_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpqa_test_df = pd.read_csv(\"altered_gpqa_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_strings = [\"A\", \"B\", \"C\", \" D\"]\n",
    "token_ids = [tokenizer.encode(s)[1] for s in answer_strings]\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model_outputs = []\n",
    "\n",
    "for i in tqdm(range(gpqa_test_df.shape[0])):\n",
    "# for i in range(1):\n",
    "    question = gpqa_test_df.loc[i, 'Dataset Question']\n",
    "    choices = gpqa_test_df.loc[i, 'context']\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will be asked a question. Answer with the letter of your answer choice; that is, only state 'A', 'B', 'C', or 'D'.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question + '\\n' + choices\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False) + \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        tokenized_prompt[\"input_ids\"],\n",
    "        max_new_tokens = 1,\n",
    "        do_sample=False,   # Greedy decoding\n",
    "        return_dict_in_generate=True,  # Get additional generation info\n",
    "        output_scores=True,  # Get the scores/logits\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    logits = outputs.scores[0][0] # Logit for generation token\n",
    "\n",
    "    logits = logits.cpu()\n",
    "    relevant_logits = [logits[token_ids[i]] for i in range(4)]\n",
    "\n",
    "    llm_output = answer_strings[np.argmax(np.array(relevant_logits))]\n",
    "\n",
    "    correct_answer = \" \" + gpqa_test_df.loc[i, 'answer']\n",
    "\n",
    "    model_outputs.append(llm_output)\n",
    "\n",
    "    if llm_output == correct_answer:\n",
    "        correct += 1\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If testing base model instead of finetuned model\n",
    "\"\"\" \n",
    "gpqa_test_df['base_answer'] = model_outputs\n",
    "gpqa_test_df.to_csv(\"04_GPQA/gpqa_base.csv\")\n",
    "\"\"\"\n",
    "\n",
    "gpqa_test_df['finetuned_answer'] = model_outputs\n",
    "gpqa_test_df.to_csv(\"04_GPQA/gpqa_finetuned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
